# Intent Transformer Module

**AI model for predicting user purchase intent from browsing behavior**

---

## ğŸ“ Files in This Directory

### **Core Files (4 files)**

| File | Purpose | Lines |
|------|---------|-------|
| `intent_transformer.py` | **Model Definition** - IntentTransformer class with BERT + Transformer | 108 |
| `train_intent_comprehensive.py` | **Training Script** - Full training with metrics tracking | 611 |
| `evaluate_intent_model.py` | **Evaluation Script** - Test set evaluation | 349 |
| `__init__.py` | **Package Init** - Makes this a Python package | 0 |

**Total**: 4 essential files, ~1,068 lines of code

---

## ğŸš€ Quick Usage

### **Training**
```bash
# From project root
python scripts/train_intent_quick.py --train

# Or directly
python src/models/intent_transformer/train_intent_comprehensive.py
```

### **Evaluation**
```bash
# From project root
python scripts/train_intent_quick.py --evaluate

# Or directly
python src/models/intent_transformer/evaluate_intent_model.py
```

### **Using the Model in Code**
```python
from src.models.intent_transformer.intent_transformer import IntentTransformer

# Initialize model
model = IntentTransformer(
    vocab_size_act=5,      # Number of action types
    embedding_dim_act=16,  # Action embedding dimension
    d_model=784,           # Combined embedding dimension (768 BERT + 16 action)
    nhead=4,               # Number of attention heads
    num_encoder_layers=2,  # Number of transformer layers
    dropout=0.1            # Dropout rate
)

# Forward pass
intent_logits, urgency_logits, nav_depth_logits = model(
    product_descriptions,  # Tokenized product descriptions
    action_indices,        # Encoded action indices
    attention_mask         # Attention mask for BERT
)
```

---

## ğŸ§  Model Architecture

```
User Session (sequence of 8 actions)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Product Descriptions (tokenized)       â”‚
â”‚  â†’ DistilBERT                           â”‚
â”‚  â†’ [CLS] embeddings (768-dim)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Actions (view, add_to_cart, etc.)     â”‚
â”‚  â†’ Action Embedding (16-dim)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Combined Embeddings (784-dim)
    â†“
Positional Encoding
    â†“
Transformer Encoder (2 layers, 4 heads)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Task Prediction Heads:          â”‚
â”‚  â€¢ Intent: Low/Medium/High (3 classes) â”‚
â”‚  â€¢ Urgency: 0-1 (continuous)           â”‚
â”‚  â€¢ Nav Depth: 0-1 (continuous)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Model Details

### **Input**
- **Product Descriptions**: Tokenized text (max 16 tokens per product)
- **Actions**: Categorical (view, add_to_cart, search, purchase)
- **Sequence Length**: 8 past actions

### **Output**
- **Intent Score**: Probability distribution over low/medium/high intent
- **Urgency Level**: Float in [0, 1] indicating purchase timing
- **Navigation Depth**: Float in [0, 1] indicating engagement level

### **Training**
- **Loss**: Combined CrossEntropy (intent) + MSE (urgency) + MSE (nav_depth)
- **Optimizer**: AdamW with learning rate 5e-4
- **Batch Size**: 8 (configurable in config.yaml)
- **Epochs**: 10 (configurable)

### **Performance**
- **Accuracy**: 80-85% on test set
- **Precision**: 0.78-0.84
- **Recall**: 0.80-0.85
- **F1-Score**: 0.79-0.84

---

## ğŸ”§ Configuration

Edit `config/config.yaml` to adjust hyperparameters:

```yaml
models:
  intent_transformer:
    model_name: "distilbert-base-uncased"
    max_seq_length: 8          # History window
    max_desc_length: 16        # Product description length
    batch_size: 8              # Batch size
    learning_rate: 5e-4        # Learning rate (0.0005)
    num_epochs: 10             # Training epochs
    dropout: 0.2               # Dropout rate
```

---

## ğŸ“ˆ Training Output

```
ğŸ‹ï¸  Starting training for 10 epochs...
----------------------------------------------------------------------
Epoch [ 1/10] | Train Acc: 0.6234 | Val Acc: 0.6891 | Val F1: 0.6754
Epoch [ 2/10] | Train Acc: 0.7456 | Val Acc: 0.7623 | Val F1: 0.7489 âœ“ Best!
...
Epoch [10/10] | Train Acc: 0.8756 | Val Acc: 0.8512 | Val F1: 0.8401

ğŸ“Š FINAL EVALUATION
  Accuracy:  0.8512 (85.12%)
  Precision: 0.8489
  Recall:    0.8512
  F1-Score:  0.8401
```

---

## ğŸ“ Generated Files

After training, these files are created:

```
models/
â”œâ”€â”€ intent_transformer_finetuned_multi.pth    # Trained model weights
â”œâ”€â”€ intent_encoders.pkl                        # Action & intent encoders
â””â”€â”€ distilbert_tokenizer/                     # BERT tokenizer files

results/
â”œâ”€â”€ intent_transformer_results.json            # Training metrics
â”œâ”€â”€ intent_test_results.json                   # Test metrics
â””â”€â”€ confusion_matrix.png                       # Visualization
```

---

## ğŸ” File Descriptions

### **`intent_transformer.py`**
Main model definition containing:
- `PositionalEncoding` class for temporal information
- `IntentTransformer` class (main model)
- Multi-head attention mechanism
- Three prediction heads (intent, urgency, nav_depth)

### **`train_intent_comprehensive.py`**
Comprehensive training script with:
- Data loading from temporal splits
- Intent label creation from user behavior
- Sequence preparation with BERT tokenization
- Training loop with early stopping
- Validation and metrics tracking
- Model checkpointing
- Results saving to JSON

### **`evaluate_intent_model.py`**
Evaluation script with:
- Model loading from checkpoint
- Test set preparation
- Inference on test data
- Comprehensive metrics calculation
- Confusion matrix generation
- Results visualization and saving

### **`__init__.py`**
Empty file that makes this directory a Python package, allowing imports like:
```python
from src.models.intent_transformer.intent_transformer import IntentTransformer
```

---

## ğŸ§ª Testing

To test the model:

```python
import torch
from src.models.intent_transformer.intent_transformer import IntentTransformer

# Create dummy data
batch_size = 4
seq_len = 8
desc_len = 16

product_descriptions = torch.randint(0, 1000, (batch_size, seq_len, desc_len))
action_indices = torch.randint(0, 5, (batch_size, seq_len))
attention_mask = torch.ones((batch_size, seq_len, desc_len))

# Initialize model
model = IntentTransformer(vocab_size_act=5)
model.eval()

# Forward pass
with torch.no_grad():
    intent, urgency, nav_depth = model(product_descriptions, action_indices, attention_mask)
    
print(f"Intent shape: {intent.shape}")        # [4, 3] - 3 classes
print(f"Urgency shape: {urgency.shape}")      # [4, 1]
print(f"Nav depth shape: {nav_depth.shape}")  # [4, 1]
```

---

## ğŸ“š Related Documentation

- **Training Guide**: `docs/INTENT_TRANSFORMER_TRAINING.md`
- **Quick Start**: `docs/INTENT_TRAINING_QUICKSTART.md`
- **Project Structure**: `PROJECT_STRUCTURE.md`
- **Main README**: `README.md`

---

## ğŸ”„ Changelog

### **v1.0** (Current)
- âœ… Core model implementation
- âœ… Comprehensive training script
- âœ… Evaluation with metrics
- âœ… Multi-task learning (intent + urgency + nav_depth)
- âœ… BERT integration for semantic understanding
- âœ… Cleaned up redundant files

### **Previous**
- âŒ Removed `model.py` (duplicate)
- âŒ Removed `tune_intent_model.py` (outdated imports)

---

## ğŸ’¡ Tips

1. **Use GPU if available**: Edit `config/config.yaml` to set `device: "cuda"`
2. **Monitor training**: Watch validation accuracy to prevent overfitting
3. **Adjust batch size**: Reduce if running out of memory
4. **Check data quality**: Ensure Category Name is properly linked in preprocessing

---

**Last Updated**: 2025-10-02  
**Status**: âœ… Production Ready  
**Maintained By**: SynchroChain Team


