# üöÄ SynchroChain Deployment Guide

**Complete deployment instructions for the AI-driven supply chain optimization system**

---

## üìã System Overview

SynchroChain is a three-layer AI system that predicts customer intent and optimizes supply chain decisions in real-time:

```
User Browsing ‚Üí Intent Transformer ‚Üí GNN ‚Üí PPO Agent ‚Üí Fulfillment Decision
```

### **Architecture Layers:**
1. **Presentation Layer**: Streamlit Dashboard
2. **Application Layer**: Intent Transformer + GNN + PPO Agent + Orchestrator  
3. **Data Layer**: Raw data ‚Üí Processed data ‚Üí Feature store

---

## ‚ö° Quick Start (5 Minutes)

### **1. Environment Setup**
```bash
# Clone and navigate to project
cd SynchroChain

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### **2. Run the System**
```bash
# Test the complete system
python scripts/simple_integration_test.py

# Launch the dashboard
streamlit run src/dashboard/streamlit_app.py

# Test individual components
python src/system/orchestrator.py
```

### **3. Access the Dashboard**
- Open browser to: `http://localhost:8501`
- Navigate through: Dashboard ‚Üí AI Predictions ‚Üí Analytics ‚Üí Settings

---

## üèóÔ∏è Complete Build Process

### **Step 1: Data Preparation**
```bash
# Preprocess raw data into temporal splits
python src/data/preprocessing.py
```

**Expected Output:**
- `data/processed/processed_access_logs.csv`
- `data/processed/gnn_nodes.csv` & `gnn_edges.csv`
- `data/processed/temporal_splits/` (train/val/test)

### **Step 2: Train AI Models**

#### **Intent Transformer (69% Accuracy)**
```bash
# Quick training (recommended)
python scripts/train_intent_quick.py --train --evaluate

# Alternative: Direct training
python src/models/intent_transformer/train_intent_comprehensive.py
```

#### **Supply Chain GNN**
```bash
python src/models/gnn/run_enhanced_gnn.py
```

#### **PPO RL Agent (21.3% Improvement)**
```bash
python src/models/agents/ppo_trainer.py
```

#### **Train All Models**
```bash
# Complete pipeline training
python scripts/train_all_models.py
```

### **Step 3: System Integration**
```bash
# Test full system integration
python scripts/simple_integration_test.py

# Expected: 100% success rate, <1s processing time
```

### **Step 4: Launch Dashboard**
```bash
# Start the web interface
streamlit run src/dashboard/streamlit_app.py
```

---

## üéØ Performance Benchmarks

### **Achieved Results:**
| Component | Metric | Value | Status |
|-----------|--------|-------|--------|
| **Intent Transformer** | Accuracy | 69% | ‚úÖ 2x better than baseline |
| **PPO Agent** | Training Improvement | 21.3% | ‚úÖ Converged successfully |
| **System Performance** | Processing Time | 0.003s | ‚úÖ Ultra-fast |
| **System Performance** | Throughput | 357 req/s | ‚úÖ High performance |
| **Integration Test** | Success Rate | 100% | ‚úÖ All systems operational |

### **PPO Agent Metrics:**
- **Average Reward**: 842.72
- **Cost Efficiency**: 3.118
- **Risk Mitigation**: 52.2%
- **Action Distribution**: 77% Priority, 18% Expedited, 5% Standard

---

## üè≠ Production Deployment

### **System Requirements**
- **Python**: 3.11+
- **Memory**: 4GB+ RAM
- **Storage**: 2GB+ available space
- **CPU**: Intel/AMD x64 (GPU optional)

### **Dependencies**
```txt
torch>=1.9.0
torch-geometric>=2.0.0
transformers>=4.20.0
streamlit>=1.20.0
gymnasium>=0.28.0
plotly>=5.0.0
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
```

### **Environment Variables**
```bash
# Optional configuration
export SYNCHROCHAIN_CONFIG_PATH="config/config.yaml"
export SYNCHROCHAIN_LOG_LEVEL="INFO"
export SYNCHROCHAIN_MODEL_PATH="models/"
```

### **Docker Deployment (Optional)**
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8501

CMD ["streamlit", "run", "src/dashboard/streamlit_app.py"]
```

---

## üîß Configuration

### **Model Configuration (`config/config.yaml`)**
```yaml
models:
  intent_transformer:
    max_seq_length: 8
    batch_size: 32
    learning_rate: 3e-4
    num_epochs: 30
    
  gnn:
    hidden_dim: 64
    num_layers: 3
    learning_rate: 0.00033
    
  ppo:
    learning_rate: 3e-4
    epochs: 100
    rollout_steps: 2048
    gamma: 0.99
```

### **Performance Tuning**
- **CPU Optimization**: Models are optimized for Intel UHD Graphics
- **Memory Usage**: ~2GB for full system
- **Batch Processing**: Configurable batch sizes for throughput
- **Caching**: Streamlit caching for model loading

---

## üìä Usage Examples

### **1. Programmatic API**
```python
from src.system.orchestrator import ModelOrchestrator

# Initialize system
orchestrator = ModelOrchestrator()

# User session data
user_session = [
    {'product_name': 'Laptop', 'action': 'view', 'timestamp': '2024-01-01 10:00:00'},
    {'product_name': 'Laptop', 'action': 'add_to_cart', 'timestamp': '2024-01-01 10:05:00'}
]

# Supply chain context
supply_context = {
    'warehouse_location': 'US_WEST',
    'current_inventory': 150,
    'historical_delay_rate': 0.15
}

# Get AI decision
results = orchestrator.orchestrate_full_pipeline(user_session, supply_context)
print(f"Decision: {results['final_decision']['action_name']}")
```

### **2. Web Dashboard**
1. Navigate to `http://localhost:8501`
2. Go to "AI Predictions" tab
3. Configure user session and supply context
4. Click "Run AI Prediction"
5. View intent analysis, risk assessment, and final decision

### **3. Batch Processing**
```python
# Process multiple sessions
sessions = [session1, session2, session3]
results = []

for session in sessions:
    result = orchestrator.orchestrate_full_pipeline(session, supply_context)
    results.append(result)
```

---

## üõ†Ô∏è Troubleshooting

### **Common Issues**

#### **1. Model Loading Errors**
```bash
# Issue: Models not found
# Solution: Ensure models are trained
python scripts/train_all_models.py
```

#### **2. Unicode Encoding Errors**
```bash
# Issue: Emoji display problems on Windows
# Solution: Use simple_integration_test.py instead of full test
python scripts/simple_integration_test.py
```

#### **3. Memory Issues**
```bash
# Issue: Out of memory during training
# Solution: Reduce batch size in config.yaml
batch_size: 16  # Instead of 32
```

#### **4. Slow Performance**
```bash
# Issue: Slow processing times
# Solution: Check system resources and model caching
# Models should load once and be cached
```

### **Verification Commands**
```bash
# Check system status
python scripts/simple_integration_test.py

# Verify data pipeline
ls data/processed/

# Check model files
ls models/

# Test individual components
python src/system/orchestrator.py
```

---

## üìà Monitoring & Maintenance

### **Performance Monitoring**
- **Processing Time**: Should be <1s per request
- **Memory Usage**: Monitor for memory leaks
- **Accuracy**: Track prediction accuracy over time
- **Error Rate**: Should be <1%

### **Model Updates**
```bash
# Retrain models with new data
python scripts/train_all_models.py

# Update configuration
# Edit config/config.yaml and restart system
```

### **Log Files**
- System logs: `logs/synchrochain.log`
- Training logs: `results/`
- Error tracking: Built into orchestrator

---

## üéì Advanced Usage

### **Custom Model Integration**
1. Implement new model in `src/models/`
2. Update orchestrator in `src/system/orchestrator.py`
3. Add configuration to `config/config.yaml`
4. Test with integration suite

### **API Development**
```python
# Create REST API wrapper
from flask import Flask, request, jsonify
from src.system.orchestrator import ModelOrchestrator

app = Flask(__name__)
orchestrator = ModelOrchestrator()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    results = orchestrator.orchestrate_full_pipeline(
        data['user_session'], 
        data['supply_context']
    )
    return jsonify(results)
```

### **Scaling Considerations**
- **Horizontal Scaling**: Deploy multiple instances behind load balancer
- **Model Serving**: Use dedicated model serving infrastructure
- **Database Integration**: Connect to production databases
- **Real-time Processing**: Implement streaming data pipeline

---

## üìö Additional Resources

### **Documentation**
- **Architecture**: `docs/architecture.md`
- **API Reference**: `docs/api_reference.md`
- **User Guide**: `docs/user_guide.md`
- **Project Structure**: `PROJECT_STRUCTURE.md`

### **Training Guides**
- **Intent Transformer**: `docs/INTENT_TRANSFORMER_TRAINING.md`
- **Quick Start**: `QUICK_START.md`
- **Full README**: `README.md`

### **Results & Analysis**
- **Performance Results**: `results/`
- **Training Plots**: `results/*.png`
- **Model Metrics**: `results/*.json`

---

## üéâ Success Criteria

Your SynchroChain deployment is successful when:

- ‚úÖ **Integration Test**: 100% pass rate
- ‚úÖ **Performance**: <1s processing time
- ‚úÖ **Dashboard**: Accessible at localhost:8501
- ‚úÖ **AI Pipeline**: Intent ‚Üí GNN ‚Üí PPO ‚Üí Decision
- ‚úÖ **Models**: PPO agent trained with 21.3% improvement
- ‚úÖ **Throughput**: >300 requests/second

---

## ü§ù Support & Contributing

### **Getting Help**
1. Check troubleshooting section above
2. Run diagnostic: `python scripts/simple_integration_test.py`
3. Review logs in `logs/` directory
4. Check model files in `models/` directory

### **Contributing**
1. Fork the repository
2. Create feature branch
3. Add tests for new functionality
4. Update documentation
5. Submit pull request

---

**üöÄ Congratulations! Your SynchroChain system is ready for production deployment!**

*Built with ‚ù§Ô∏è for smarter supply chains*
